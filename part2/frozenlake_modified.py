import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle

total_win = 0

def run(episodes, is_training=True, render=False):
    global total_win

    self_map = [
    "SFFFFFFF",
    "FFFFFFFF",
    "FFHFFFFF",
    "FFFFFHFF",
    "FFFFFFFH",
    "HFFFFFFF",
    "FFFFFFHF",
    "FFFHFFFG",
    ]

    env = gym.make('FrozenLake-v1', desc=self_map, is_slippery=True, render_mode='human' if render else None)

    if(is_training):
        # åˆå§‹ Q table ç‚º 0.1 -> é¼“å‹µæ¢ç´¢
        q = np.zeros((env.observation_space.n, env.action_space.n))
    else:
        try:
            f = open('frozen_lake8x8.pkl', 'rb')
            q = pickle.load(f)
            f.close()
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆï¼")
            return
    
    # 1. å­¸ç¿’ç‡ï¼šå‹•æ…‹è¡°æ¸› 
    start_lr = 0.7
    min_lr = 0.01
    
    # 2. çæ‡²æ©Ÿåˆ¶
    hole_penalty = -0.2
    step_penalty = -0.001
    dest_reward = 1.5

    # 3. Gamma
    discount_factor_g = 0.97

    # 4. æ¢ç´¢ç‡ï¼šæŒ‡æ•¸è¡°æ¸›
    epsilon = 1.0
    min_exploration_rate = 0.01
    epsilon_decay_rate = 0.9995
    
    rng = np.random.default_rng()
    rewards_per_episode = np.zeros(episodes)

    for i in range(episodes):
        state = env.reset()[0]
        terminated = False
        truncated = False
        
        # === å‹•æ…‹å­¸ç¿’ç‡å…¬å¼ ===
        # éš¨è‘—è¨“ç·´æ¬¡æ•¸å¢åŠ ï¼ŒLR é€æ¼¸è®Šå°ï¼Œæ¸›å°‘å¾ŒæœŸçš„éœ‡ç›ª
        current_lr = max(min_lr, start_lr * (1 - i / (episodes * 0.9)))

        while(not terminated and not truncated):
            if is_training and rng.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q[state,:])

            new_state, reward, terminated, truncated, _ = env.step(action)

            # çå‹µæ©Ÿåˆ¶
            if terminated and reward == 1:
                custom_reward = dest_reward    # çµ‚é»
            elif terminated and reward == 0:
                custom_reward = hole_penalty   # æ‰æ´ 
            else:
                custom_reward = step_penalty   # èµ°è·¯

            # å¢åŠ é”æˆç›®æ¨™çš„å­¸ç¿’ç‡
            if custom_reward > 0:
                current_lr = 0.9
            if is_training:
                # è™•ç† Truncated (è¶…æ™‚) : ä¿ç•™æœªä¾†åƒ¹å€¼
                if truncated:
                    target = custom_reward + discount_factor_g * np.max(q[new_state,:])
                elif terminated:
                    target = custom_reward
                else:
                    target = custom_reward + discount_factor_g * np.max(q[new_state,:])

                # æ›´æ–° Q Table
                q[state,action] = q[state,action] + current_lr * (target - q[state,action])

            state = new_state
            
            if reward == 1:
                rewards_per_episode[i] = 1

        # è¡°æ¸›æ¢ç´¢ç‡
        epsilon = max(min_exploration_rate, epsilon * epsilon_decay_rate)

    env.close()

    # ç¹ªåœ–
    sum_rewards = np.zeros(episodes)
    for t in range(episodes):
        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])
    plt.plot(sum_rewards)
    plt.title(f'Target 70%: LR Decay, Hole -0.1')
    plt.savefig('frozen_lake8x8.png')
    
    if is_training == False:
        win = np.sum(rewards_per_episode) / episodes * 100
        print(f"âœ… Success Rate: {win:.2f}% ({int(np.sum(rewards_per_episode))} / {episodes} episodes)")
        total_win += win

    if is_training:
        f = open("frozen_lake8x8.pkl","wb")
        pickle.dump(q, f)
        f.close()

if __name__ == '__main__':
    print("ğŸ”¥ é–‹å§‹ä¿®æ­£å¾Œè¨“ç·´ (LR Decay, Hole -0.1)...")
    run(15000, is_training=True, render=False)

    print("\ntesting section (1000 times each round):")
    for i in range(0, 10):
        run(1000, is_training=False, render=False)
    
    print(f"\nFinal average success rate: {total_win / 10:.2f}%")
